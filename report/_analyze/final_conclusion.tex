\section{Insights and Conclusion}

This section presents a comprehensive analysis of the experimental results obtained from fine-tuning three transformer-based models: \textbf{T5-base}, \textbf{BART-base}, and \textbf{Flan-T5-small}. Each model was fine-tuned on three natural language processing (NLP) tasks—\textbf{Question Answering}, \textbf{Machine Translation}, and \textbf{Sentiment Classification}—using three techniques: \textbf{Full Finetuning}, \textbf{LoRA (Low-Rank Adaptation)}, and \textbf{Adapter Tuning}.

\subsection*{1. Question Answering (SQuAD 1.0)}
\textbf{Metrics Used:} Exact Match (EM), F1 Score

\textbf{Key Observations:}
\begin{itemize}
    \item For the \textbf{T5-base} model, \textbf{LoRA} achieved the highest performance among all configurations, with an EM of 94.31\% and F1 of 95.73\%. This result surpasses both full finetuning and adapter tuning, indicating that LoRA can efficiently capture task-relevant information with fewer trainable parameters.
    
    \item The \textbf{Adapter} approach also performs competitively on T5-base (EM: 92.69\%, F1: 94.42\%), making it a lightweight yet strong alternative to full finetuning.
    
    \item \textbf{BART-base} shows mixed results. While LoRA improves performance significantly over full finetuning (EM from 69.38\% to 84.17\%), adapter tuning fails catastrophically (EM: 36.57\%, F1: 40.97\%), suggesting that adapter layers are not well-aligned with BART’s architecture for QA tasks.
    
    \item \textbf{Flan-T5-small} exhibits a noteworthy trend where adapter tuning (F1: 86.35\%) outperforms full finetuning (F1: 84.94\%). This suggests that smaller, instruction-tuned models may be more amenable to modular fine-tuning techniques.
\end{itemize}

\textbf{Conclusion:} 
\textbf{LoRA stands out as the best method for question answering}, particularly with T5-based models, achieving both high accuracy and computational efficiency. Adapter tuning also performs well on encoder-decoder models like T5, but its application on BART requires further investigation due to poor compatibility. Interestingly, even small models like \textbf{Flan-T5-small} can rival larger ones when equipped with suitable fine-tuning techniques.

\subsection*{2. Machine Translation (WMT16 English-German)}
\textbf{Metrics Used:} BLEU Score, Cosine Similarity

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{T5-base} consistently performs the best across all tuning methods in translation tasks. Notably, \textbf{Adapter tuning slightly outperforms full finetuning} in BLEU score (0.2355 vs. 0.2259), showing its strength in modular and efficient learning.
    
    \item \textbf{BART-base} performs the worst across all methods, with BLEU dropping to nearly zero with adapters (0.0001). This performance collapse suggests that the translation capabilities of BART may not generalize well under parameter-efficient tuning.
    
    \item \textbf{Flan-T5-small} delivers moderate results, with full finetuning achieving the best outcome for this model (BLEU: 0.1383). Adapter and LoRA tuning both yield subpar results compared to T5-base, likely due to the model’s smaller capacity and lack of extensive multilingual pretraining.
\end{itemize}

\textbf{Conclusion:} 
In translation tasks, \textbf{T5-base with Adapters is surprisingly the most effective}, combining high BLEU scores with modular training benefits. LoRA does not perform well for sequence generation tasks, possibly due to its limited expressiveness in controlling decoder behavior. \textbf{BART-base exhibits structural limitations for parameter-efficient methods}, making it unsuitable for lightweight translation fine-tuning without architectural modifications.

\subsection*{3. Sentiment Classification (IMDB)}
\textbf{Metrics Used:} Accuracy, Precision, Recall, F1 Score

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{T5-base with full finetuning} attains the highest performance overall (F1: 0.9449), demonstrating its capability to learn detailed classification boundaries with end-to-end training.
    
    \item \textbf{Flan-T5-small with LoRA} astonishingly matches the performance of T5-base full finetuning (F1: 0.9449). This indicates that even small models can be powerful classifiers when equipped with efficient tuning techniques like LoRA.
    
    \item \textbf{BART-base with LoRA} shows better performance than its full finetuning variant (F1: 0.9192 vs. 0.9051), suggesting that LoRA enhances generalization in this model.
    
    \item Adapters on \textbf{BART-base} again perform poorly (F1: 0.6967), reinforcing the incompatibility observed in the QA task. In contrast, \textbf{T5-base and Flan-T5-small with adapters} perform well, indicating the importance of architectural synergy.
\end{itemize}

\textbf{Conclusion:} 
\textbf{LoRA demonstrates exceptional utility in sentiment analysis}, especially with smaller models like Flan-T5-small, delivering top-tier results with minimal computational overhead. Adapter tuning continues to perform strongly on T5-based models but is ineffective on BART. \textbf{Full finetuning is still a gold standard for absolute performance}, though it is more resource-intensive.

\subsection*{4. General Observations and Strategic Takeaways}

\begin{itemize}
    \item Across all tasks, \textbf{T5-base is the most robust and consistent model}, showing strong results with every fine-tuning method.
    
    \item \textbf{LoRA performs best in classification and QA}, but underperforms in translation. It appears to be better suited for token-level prediction tasks rather than sequence generation.
    
    \item \textbf{Adapter tuning is highly effective when paired with compatible architectures}, particularly encoder-decoder structures like T5 and Flan-T5. It fails significantly on BART, pointing to a mismatch in design.
    
    \item \textbf{Full finetuning remains the most reliable but computationally expensive option}. In constrained environments, LoRA and Adapters are powerful alternatives.
    
    \item \textbf{Flan-T5-small stands out as a cost-effective yet competitive model}, especially when paired with LoRA, making it ideal for real-world applications with limited resources.
\end{itemize}

\subsection*{5. Recommended Configurations by Task}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|p{8cm}|}
\hline
\textbf{Task} & \textbf{Recommended Setup} & \textbf{Rationale} \\
\hline
Question Answering & T5-base + LoRA & Highest EM and F1, excellent accuracy-efficiency tradeoff \\
\hline
Translation & T5-base + Adapters & Best BLEU score, modular and efficient for generation tasks \\
\hline
Sentiment Classification & Flan-T5-small + LoRA & Matches T5-base full finetuning with lower computational cost \\
\hline
\end{tabular}
\caption{Recommended model and tuning method combinations per task}
\end{table}

\newpage
