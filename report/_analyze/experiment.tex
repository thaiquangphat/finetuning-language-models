\section{Fine-Tuning Results Comparison}

\subsection{Overview of Experimental Settings}

To investigate the effectiveness of various fine-tuning techniques on large language models (LLMs), we conducted a comprehensive evaluation involving three transformer-based models: \textbf{T5-base}, \textbf{BART-base}, and \textbf{Flan-T5-small}. Each model was fine-tuned using three distinct methods: \textbf{Full Finetuning}, \textbf{Low-Rank Adaptation (LoRA)}, and \textbf{Adapters}. These fine-tuning strategies were applied across three representative NLP tasks—\textbf{Question Answering}, \textbf{Machine Translation}, and \textbf{Sentiment Classification}—chosen to cover a range of language understanding capabilities.

\subsection{Evaluation Criteria}

Each task provides multiple metrics for evaluation; however, to maintain a consistent and interpretable comparison across different methods and models, we select a primary metric for each task based on its relevance and widespread usage:

\begin{itemize}
    \item For \textbf{Question Answering (SQuAD 1.0)}, we focus on the \textbf{F1 Score}, which captures both precision and recall, and is especially important for extractive QA tasks.
    \item For \textbf{Translation (WMT16 English-German)}, we use the \textbf{BLEU Score}, a standard metric that quantifies translation quality based on n-gram overlap with reference translations.
    \item For \textbf{Sentiment Classification (IMDB)}, we again rely on the \textbf{F1 Score}, as it provides a balanced measure of classification performance in the presence of potential label imbalance.
\end{itemize}

\subsection{Best Performing Configurations}

\begin{table}[H]
    \centering
    \begin{tabular}{|>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{3cm}|}
    \hline
    \textbf{Model} & \textbf{Method} & \textbf{BLEU Score} & \textbf{Cosine Similarity} \\
    \hline
    \multirow{3}{*}{T5-base} 
    & Full Finetuning & 0.2259 & 0.4400 \\
    & LoRA             & 0.1999 & 0.3812 \\
    & Adapters         & 0.2355 & 0.4256 \\
    \hline
    \multirow{3}{*}{BART-base} 
    & Full Finetuning & 0.1197 & 0.3499 \\
    & LoRA             & 0.0491 & 0.2423 \\
    & Adapters         & 0.0001 & 0.0642 \\
    \hline
    \multirow{3}{*}{Flan-T5-small} 
    & Full Finetuning & 0.1383 & 0.3559 \\
    & LoRA             & 0.0762 & 0.2807 \\
    & Adapters         & 0.1095 & 0.3287 \\
    \hline
    \end{tabular}
    \caption{Translation}
\end{table}
    
\begin{table}[H]
    \centering
    \begin{tabular}{|>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{3cm}|}
    \hline
    \textbf{Model} & \textbf{Method} & \textbf{Exact Match (EM)} & \textbf{F1 Score} \\
    \hline
    \multirow{3}{*}{T5-base} 
    & Full Finetuning & 85.2312\% & 87.3847\% \\
    & LoRA             & 94.3136\% & 95.7342\% \\
    & Adapters         & 92.6941\% & 94.4193\% \\
    \hline
    \multirow{3}{*}{BART-base} 
    & Full Finetuning & 69.3850\% & 73.6033\% \\
    & LoRA             & 84.1681\% & 87.0804\% \\
    & Adapters         & 36.5725\% & 40.9718\% \\
    \hline
    \multirow{3}{*}{Flan-T5-small} 
    & Full Finetuning & 82.5057\% & 84.9403\% \\
    & LoRA             & 74.2509\% & 76.9574\% \\
    & Adapters         & 84.1039\% & 86.3504\% \\
    \hline
    \end{tabular}
    \caption{Question Answering}
\end{table}
    
\begin{table}[H]
    \centering
    \begin{tabular}{|>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|}
    \hline
    \textbf{Model} & \textbf{Method} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
    \hline
    \multirow{3}{*}{T5-base} 
    & Full Finetuning & 0.9425 & 0.9667 & 0.9239 & 0.9449 \\
    & LoRA            & 0.7063 & 0.7029 & 0.7335 & 0.7179 \\
    & Adapters        & 0.9084 & 0.8946 & 0.9298 & 0.9119 \\
    \hline
    \multirow{3}{*}{BART-base} 
    & Full Finetuning & 0.9021 & 0.8942 & 0.9163 & 0.9051 \\
    & LoRA            & 0.9184 & 0.9274 & 0.9112 & 0.9192 \\
    & Adapters        & 0.6074 & 0.5744 & 0.8853 & 0.6967 \\
    \hline
    \multirow{3}{*}{Flan-T5-small} 
    & Full Finetuning & 0.9142 & 0.9137 & 0.9183 & 0.9160 \\
    & LoRA            & 0.9425 & 0.9667 & 0.9239 & 0.9449 \\
    & Adapters        & 0.8895 & 0.8582 & 0.9380 & 0.8963 \\
    \hline
    \end{tabular}
    \caption{Text Sentiment}
\end{table}

The results indicate that different combinations of models and fine-tuning methods yield optimal performance depending on the task. Below, we summarize the best-performing configuration for each task based on the selected metric:

\begin{itemize}
    \item \textbf{Question Answering}: The highest F1 Score was achieved by \textbf{T5-base fine-tuned using LoRA}, reaching \textbf{95.73\%}. This suggests that LoRA is highly effective in enabling parameter-efficient learning while maintaining strong performance on extractive QA.
    
    \item \textbf{Translation}: The best BLEU Score was obtained by \textbf{T5-base with Adapters}, achieving \textbf{0.2355}. This highlights the capability of adapter-based tuning to support generative tasks like translation with competitive results while reducing the need for full model updates.
    
    \item \textbf{Sentiment Classification}: Both \textbf{T5-base with Full Finetuning} and \textbf{Flan-T5-small with LoRA} attained the top F1 Score of \textbf{0.9449}, indicating that both full parameter updates and efficient low-rank adaptation can be viable strategies for binary classification tasks.
\end{itemize}

These findings demonstrate that no single method dominates across all tasks; rather, the effectiveness of a fine-tuning strategy depends heavily on the model architecture and the nature of the downstream task. Notably, \textbf{T5-base} consistently appears in the best-performing combinations, reflecting its versatility and robustness across different NLP domains.

For more detailed comparison, key insigts and conclusion, we would say in the next part.

\newpage 
