\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Transformer Encoder-Decoder Architecture}}{11}{figure.caption.8}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Multiheaded attention}}{15}{figure.caption.9}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces The Use of Feed-Forward for Transformer Based Image Captioning Tasks}}{16}{figure.caption.10}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces History and development of language models}}{25}{figure.caption.11}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces A Neural Language Model}}{32}{figure.caption.12}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Architecture of a Transformer}}{38}{figure.caption.13}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces History of Transformers}}{39}{figure.caption.14}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Number of parameters of popular Neural Language Models}}{41}{figure.caption.15}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Architecture of Generative Pre-trained Transformer}}{43}{figure.caption.16}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Bidirectional Encoder Representation from Transformer's Architecture}}{44}{figure.caption.17}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces Understanding language using XLNET with autoregressive pre-training}}{45}{figure.caption.18}%
\contentsline {figure}{\numberline {4.9}{\ignorespaces Google Text-to-Text Transfer Transformer(T5)}}{46}{figure.caption.19}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces From pre-trained LLM to final model ready for deployment through fine-tuning.}}{48}{figure.caption.20}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Supervised fine-tuning of a base LLM using specific (possibly private) knowledge.}}{49}{figure.caption.21}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Illustration of the full fine-tuning process, showing how all parameters of a pre-trained LLM are updated using a domain-specific dataset to create a fine-tuned model that can handle specialized queries.}}{50}{figure.caption.22}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Architectural diagram of adapter-based fine-tuning showing the integration of bottleneck adapter modules within a pre-trained language model. The left panel illustrates adapter placement within the transformer architecture, while the right panel details the internal structure of a bottleneck adapter module.}}{51}{figure.caption.23}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Comparison between standard linear projection (left) and Low-Rank Adaptation (right), illustrating how LoRA decomposes weight updates into low-rank matrices A and B that operate in parallel with frozen pre-trained weights.}}{53}{figure.caption.24}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Architecture of prefix-tuning. The left side shows the overall Transformer structure with attention and feed-forward blocks repeated L times. The right side illustrates the prefix mechanism, where trainable prefix vectors $P_K$ and $P_V$ are prepended to keys and values in the attention mechanism. The prefix undergoes reparameterization through a small MLP with up-projection, nonlinearity, and down-projection steps before being added to the attention computation.}}{55}{figure.caption.25}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces Architecture of prompt-tuning for classification tasks. The continuous soft prompts are prepended to tokenized inputs at the embedding layer. Special tokens like [CLS], [MASK], and [SEP] maintain their functionality while the model processes both the soft prompts and regular input through its layers (L1 through L12). The output is then processed by a masked language model (MLM) head and verbalizer to produce the target classification.}}{56}{figure.caption.26}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces The two-stage process of instruction tuning. Step 1 (left) illustrates the instruction dataset construction, where text-label templates and seed instructions are expanded using stronger models like ChatGPT and GPT-4 to generate diverse instruction-output pairs. Step 2 (right) shows the actual instruction tuning phase, where the constructed dataset is used to perform supervised fine-tuning on the target LLM.}}{58}{figure.caption.27}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces Complete RLHF training pipeline. The process begins with pretraining on large text corpora (left), followed by supervised fine-tuning on demonstration data. The RLHF-specific components (outlined by the dashed box) include reward model training from human comparison data and reinforcement learning optimization. The figure also shows typical data scales for each stage and examples of models that have used these techniques.}}{59}{figure.caption.28}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
