\chapter{Description}

Since 2017, the introduction of the Transformer model has significantly transformed the field of Natural Language Processing (NLP). The Transformer architecture, proposed by Vaswani et al. in the paper \textbf{``Attention Is All You Need''}, replaced traditional recurrent and convolutional networks with self-attention mechanisms. This innovation enabled efficient parallel computation, resulting in improved training speed and model performance.

Transformers have since become the foundation of state-of-the-art NLP models such as \textbf{BERT}, \textbf{GPT}, and \textbf{T5}. These models leverage massive datasets and billions to trillions of parameters, leading to breakthroughs in various NLP tasks. However, the increasing scale of LLMs has also introduced challenges related to computational efficiency, making full fine-tuning infeasible in many scenarios.

To address these challenges, researchers have developed various fine-tuning techniques that balance efficiency and performance. Parameter-efficient fine-tuning (PEFT) methods such as LoRA, adapter-based fine-tuning, and prompt tuning allow practitioners to adapt large models with minimal computational overhead. These approaches enable LLMs to be utilized in domain-specific applications while mitigating the resource constraints typically associated with full fine-tuning.

This study investigates the development and fine-tuning of Transformer-based models, examining their evolution, key milestones, and experimental applications. By evaluating different fine-tuning strategies, we aim to provide insights into how LLMs can be optimized for real-world NLP tasks without excessive computational requirements.

\newpage