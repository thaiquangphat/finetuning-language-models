\chapter{Conclusion}

This report has provided a comprehensive study of transformer-based architectures and their practical application to natural language processing tasks. By combining theoretical foundations, historical evolution, fine-tuning strategies, and empirical evaluation, we aimed to develop a deeper understanding of how large language models can be effectively trained and adapted for real-world use.

We began by exploring the general structure and mechanisms of Transformer models and followed with a discussion on the evolution of large language models (LLMs). This contextual backdrop helped ground the motivation for modern approaches to model training and adaptation.

A central focus of this report was the examination of multiple fine-tuning techniques, including \textbf{Full Finetuning}, \textbf{Adapter Tuning}, and \textbf{LoRA}. We analyzed their trade-offs in terms of efficiency, flexibility, and performance, especially in the context of emerging needs for scalable model adaptation.

To assess these methods in practice, we carried out a set of experiments across three key NLP tasks: Question Answering, Machine Translation, and Sentiment Classification. The models evaluated—\textbf{T5-base}, \textbf{BART-base}, and \textbf{Flan-T5-small}—were fine-tuned using each method, and the results were evaluated through task-specific metrics.

From the findings, it is clear that \textbf{T5-base consistently delivers strong and stable performance}, while \textbf{LoRA} is highly competitive in low-resource scenarios, especially in classification and QA. \textbf{Adapter tuning}, while effective on T5 variants, showed limitations on BART. \textbf{Flan-T5-small}, despite its smaller size, proved highly efficient and adaptable when paired with lightweight tuning techniques.

In summary, this assignment has successfully demonstrated both the theoretical underpinnings and practical implications of fine-tuning large language models. The insights derived from this study will be valuable for future research and deployment of transformer-based systems in both academic and industrial settings.

\newpage
