\section{Current Progress}

Throughout the course CO3086 - \textbf{Natural Language Processing} (Class: CC01), we have explored the development and fine-tuning of Transformer-based Large Language Models (LLMs). This assignment has provided a comprehensive understanding of how these models have evolved and the trade-offs between full and parameter-efficient fine-tuning methods.

At this stage, we have successfully:
\begin{itemize}
    \item Conducted a thorough literature review on Transformer models, starting from the foundational paper \textbf{``Attention Is All You Need''}.
    \item Explored various fine-tuning strategies, including full fine-tuning, adapter-based tuning, LoRA, and prompt tuning.
    \item Implemented multiple fine-tuning techniques using pre-trained Transformer models on real-world NLP tasks such as text classification, named entity recognition, and summarization.
    \item Analyzed the impact of different fine-tuning methods in terms of performance and computational efficiency.
\end{itemize}

Moving forward, we aim to further refine our experiments and provide deeper insights into optimizing LLMs for specific applications.
