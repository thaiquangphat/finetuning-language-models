\chapter{Abstract}

The rapid advancement of Transformer-based Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP), enabling significant improvements in tasks such as text generation, machine translation, and text summarization. However, the exponential growth in model size and complexity has introduced challenges in adapting these models efficiently for task-specific applications.

This project focuses on exploring and analyzing fine-tuning techniques that optimize large Transformer-based models while reducing computational cost. By applying different fine-tuning strategies, including full fine-tuning, adapter-based methods, and parameter-efficient fine-tuning (PEFT) approaches such as LoRA and prompt tuning, we aim to evaluate their effectiveness across multiple NLP tasks. 

Through rigorous experimentation, we assess the trade-offs between computational efficiency and model performance, providing insights into the best practices for fine-tuning LLMs in real-world applications. The findings contribute to advancing research in optimizing Transformer-based models for task-specific deployment while addressing scalability challenges.

\newpage
